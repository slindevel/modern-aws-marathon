{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練預測模型\n",
    "在這個筆記本中，我們將訓練一個深度學習模型，該模型基於歷史價格數據，學習下一個交易日是否會觸及目標價格或止損價格，以進行多頭/空頭交易。\n",
    "\n",
    "模型：\n",
    "* 模型名稱：model_long_short_predict\n",
    "* 多層感知器（MLP）（前向神經網絡）\n",
    "* 3層：輸入、隱藏、輸出\n",
    "* 二元分類\n",
    "* 輸入：收盤價，SMA（2到16），ROC（2到16）\n",
    "* 輸出：在接下來的五天內，多頭或空頭交易是否觸及了2%的利潤目標，而未觸及1.5%的止損點？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%run ./init_model.py 'model_long_short_predict'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟一： 從 S3 獲取 Day26 建立的資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# get S3 bucket\n",
    "s3bucket=!(aws s3 ls | grep algotrading- | awk  '{print $3}')\n",
    "s3bucket=s3bucket[0]\n",
    "s3bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install PyAthena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker as sage\n",
    "from sagemaker import get_execution_role\n",
    "import datetime\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "import json\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sage.Session()\n",
    "region = sess.boto_session.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyathena import connect\n",
    "conn = connect(s3_staging_dir='s3://'+s3bucket+'/results/',\n",
    "               region_name=region)\n",
    "\n",
    "df = pd.read_sql(\"SELECT * FROM algo_data.hist_data_daily;\", conn)\n",
    "df.set_index(pd.DatetimeIndex(df['dt']),inplace=True)\n",
    "del df['dt']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv('local/'+model_name+'/input/data/training/data_orig.csv')\n",
    "print(\"count=%s\" % len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "df[\"close\"].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟二：本機端（Notebook Instance環境）資料前置處理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 撰寫資料前置處理程式碼\n",
    "這段程式碼主要是用於進行金融市場數據的預處理，以生成用於機器學習或其他分析模型的輸入數據。下面是程式碼的主要步驟和功能：\n",
    "\n",
    "匯入必要的Python庫：\n",
    "numpy（作為 np 別名）：用於數字運算。\n",
    "pandas（作為 pd 別名）：用於數據處理和分析。\n",
    "talib（作為 ta 別名）：用於技術分析指標的計算。\n",
    "math：用於一些數學運算。\n",
    "\n",
    "使用Pandas的read_csv函數讀取CSV文件 data_orig_file，並將其存儲在DataFrame d 中。同時，該函數也將日期時間列 dt 設置為索引列。接著，打印了 d 的前幾行數據以檢查。\n",
    "設置了一些變數，包括 repeatCount、repeatStep、lookBack、forwardWindow、profitTarget 和 stopTarget 等，這些變數將在後續的計算中使用。\n",
    "\n",
    "創建了列名為 hData 的列表，其中包含了即將創建的DataFrame的列名。這些列包括日期時間 dt、收盤價 close、一系列移動平均值（sma）和一系列變化率（roc），以及 long 和 short 兩個列。\n",
    "創建了一個空的 tData 列表，用於存儲處理後的數據。\n",
    "開始對數據進行迴圈處理，其中包括以下步驟：\n",
    "\n",
    "提取日期時間和收盤價。\n",
    "計算移動平均值（SMA）和變化率（ROC）。\n",
    "計算特定條件下的 long 和 short 信號，並將它們添加到 inputRec 中。\n",
    "將處理好的 inputRec 添加到 tData 列表中。\n",
    "輸出 lCount 和 sCount，這些變數分別表示 long 和 short 信號的計數。\n",
    "\n",
    "創建一個新的DataFrame df1，並將處理後的數據存儲在其中。同時，將日期時間列 dt 設置為索引列，並刪除了原本的 dt 列。\n",
    "最後，將處理好的數據存儲為一個新的CSV文件 data_file，同時打印 df1 的前幾行數據和數據的總數。這個新的CSV文件包含了經過處理的金融市場數據，可以用於後續的分析或機器學習模型的訓練。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile model/{model_name}_prep.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import talib as ta\n",
    "from talib.abstract import *\n",
    "import math\n",
    "\n",
    "prefix = '/opt/ml/'\n",
    "input_path = prefix + 'input/data/training'\n",
    "\n",
    "data_orig_file = input_path+'/data_orig.csv'\n",
    "data_file = input_path+'/data.csv'\n",
    "\n",
    "d = pd.read_csv(data_orig_file,infer_datetime_format=True, parse_dates=['dt'], index_col=['dt'])\n",
    "print(d.head())\n",
    "\n",
    "repeatCount=15\n",
    "repeatStep=1\n",
    "lookBack=repeatCount*repeatStep\n",
    "forwardWindow=5\n",
    "\n",
    "profitTarget=2.0/100.0\n",
    "stopTarget=1.5/100.0\n",
    "\n",
    "iCount=lookBack\n",
    "\n",
    "# header\n",
    "hData=[\"dt\"]\n",
    "hData.append(\"close\")\n",
    "for a in range(0,repeatCount):\n",
    "    hData.append(\"sma\"+str((a+2)*repeatStep))\n",
    "for a in range(0,repeatCount):\n",
    "    hData.append(\"roc\"+str((a+2)*repeatStep))\n",
    "hData.append(\"long\")\n",
    "hData.append(\"short\")\n",
    "\n",
    "# data\n",
    "tData=[]\n",
    "\n",
    "inputs = {\n",
    "    'close': np.array(d[\"close\"])\n",
    "}\n",
    "sma=[]\n",
    "for a in range(0,repeatCount):\n",
    "    sma.append(SMA(inputs,timeperiod=(a+1)*repeatStep+1))\n",
    "roc=[]\n",
    "for a in range(0,repeatCount):\n",
    "    roc.append(ROC(inputs,timeperiod=(a+1)*repeatStep+1))\n",
    "\n",
    "closeList=d[\"close\"]\n",
    "dLen=len(d)\n",
    "n=0\n",
    "lCount=0\n",
    "sCount=0\n",
    "nCount=0\n",
    "n=0\n",
    "for idx,row in d.iterrows():\n",
    "    if n<dLen-forwardWindow-1:\n",
    "        dt1=idx\n",
    "        cl=row[\"close\"]\n",
    "        inputRec=[]\n",
    "        inputRec.append(idx)\n",
    "\n",
    "        inputRec0=[]\n",
    "\n",
    "        #close\n",
    "        inputRec0.append(cl)\n",
    "\n",
    "        #sma\n",
    "        for a in range(0,repeatCount):\n",
    "            if math.isnan(sma[a][n]):\n",
    "                inputRec0.append(cl)\n",
    "            else:\n",
    "                inputRec0.append(sma[a][n])\n",
    "\n",
    "        m1=min(inputRec0)\n",
    "        m2=max(inputRec0)\n",
    "        for a in inputRec0:\n",
    "            if m2-m1==0:\n",
    "                inputRec.append(0)\n",
    "            else:\n",
    "                inputRec.append((a-m1)/(m2-m1))\n",
    "\n",
    "        #roc\n",
    "        for a in range(0,repeatCount):\n",
    "            if math.isnan(roc[a][n]):\n",
    "                inputRec.append(0)\n",
    "            else:\n",
    "                inputRec.append(roc[a][n])\n",
    "\n",
    "        rClose=closeList[n+1:min(dLen-1,n+1+forwardWindow)].values.tolist()\n",
    "        low=min(rClose)\n",
    "        high=max(rClose)\n",
    "        \n",
    "        #long\n",
    "        long=0\n",
    "        if high>=cl+cl*profitTarget and low>=cl-cl*stopTarget:\n",
    "            long=1\n",
    "            lCount=lCount+1\n",
    "        inputRec.append(long)\n",
    " \n",
    "        #short\n",
    "        short=0\n",
    "        if low<=cl-cl*profitTarget and high<=cl+cl*stopTarget:\n",
    "            short=1\n",
    "            sCount=sCount+1\n",
    "        inputRec.append(short)\n",
    "\n",
    "        tData.append(inputRec)\n",
    "        n=n+1\n",
    "          \n",
    "print(\"lCount=%s,sCount=%s\" % (lCount,sCount))\n",
    "df1=pd.DataFrame(tData,columns=hData)\n",
    "df1.set_index(pd.DatetimeIndex(df1['dt']), inplace=True)\n",
    "del df1['dt']\n",
    " \n",
    "df1.to_csv(data_file)\n",
    "print(df1.head())\n",
    "print(\"count=%s\" % (len(df1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本機端執行上面寫的程式碼 in a Docker Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!cp model/{model_name}_prep.py model/train\n",
    "!chmod 777 model/train\n",
    "!docker build -t {model_name}_prep .\n",
    "!docker run -v $(pwd)/local/$model_name:/opt/ml --rm {model_name}_prep train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 產生訓練 & 測試資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"local/\"+model_name+\"/input/data/training/data.csv\", infer_datetime_format=True, parse_dates=['dt'], index_col=['dt'])\n",
    "print(\"totalCount=%s\" % len(df))\n",
    "\n",
    "trainCount=int(len(df)*0.4)\n",
    "dfTrain = df.iloc[:trainCount]\n",
    "dfTrain.to_csv(\"local/\"+model_name+\"/input/data/training/data_train.csv\")\n",
    "print(\"trainCount=%s\" % len(dfTrain))\n",
    "\n",
    "dfTest = df.iloc[trainCount:]\n",
    "dfTest.to_csv(\"local/\"+model_name+\"/input/data/training/data_test.csv\")\n",
    "print(\"testCount=%s\" % len(dfTest))\n",
    "dfTest.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟3: 訓練你的模型\n",
    "這邊我們保留原 blog 的程式碼來訓練模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile model/{model_name}.py\n",
    "#!/usr/bin/env python\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dropout, Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "yLen=2\n",
    "b=0\n",
    "\n",
    "# Optional\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# These are the paths to where SageMaker mounts interesting things in your\n",
    "# container.\n",
    "prefix = '/opt/ml/'\n",
    "\n",
    "input_path = prefix + 'input/data/training/data_train.csv'\n",
    "test_path = prefix + 'input/data/training/data_test.csv'\n",
    "\n",
    "output_path = os.path.join(prefix, 'output')\n",
    "model_path = os.path.join(prefix, 'model')\n",
    "\n",
    "# Process and prepare the data\n",
    "def data_process(df):\n",
    "    global yLen\n",
    "    global b\n",
    "    dataX=[]\n",
    "    dataY=[]\n",
    "    for idx,row in df.iterrows():\n",
    "        row1=[]\n",
    "        r=row[1:len(row)-yLen]\n",
    "        for a in r:\n",
    "            row1.append(a)\n",
    "        x=np.array(row1)\n",
    "        y=np.array(row[len(row)-yLen:])\n",
    "        b=len(x)\n",
    "        dataX.append(x)\n",
    "        dataY.append(y)\n",
    "    dataX=np.array(dataX).astype(np.float32)\n",
    "    dataY=np.array(dataY).astype(np.float32)\n",
    "    return dataX,dataY,b\n",
    "\n",
    "def build_classifier():\n",
    "    global b\n",
    "    global yLen\n",
    "    print(\"build_classifier:b=%s,yLen=%s\" % (b,yLen))\n",
    "    model = Sequential()\n",
    "    model.add(Dense(b, input_dim=b, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(int(b/2), kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(yLen,kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def generate_model(dataX, dataY, b):\n",
    "    model=build_classifier()\n",
    "    model.fit(dataX, dataY, epochs=100, batch_size=1)\n",
    "    scores = model.evaluate(dataX, dataY, verbose=0)\n",
    "    print(\"Training Data %s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    return model\n",
    "        \n",
    "def train():\n",
    "    print('Starting the training.')\n",
    "    try:\n",
    "        raw_data = pd.read_csv(input_path)\n",
    "        #print(raw_data)\n",
    "        X, y, b = data_process(raw_data)\n",
    "        model = generate_model(X, y, b)\n",
    "        model.save(os.path.join(model_path, 'model.h5'))\n",
    "        \n",
    "        print('Training is complete. Model saved.')\n",
    "        \n",
    "        raw_data = pd.read_csv(test_path)\n",
    "        testX, testY, b = data_process(raw_data)\n",
    "        scores = model.evaluate(testX, testY, verbose=0)\n",
    "        print(\"Test Data %s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Write out an error file. This will be returned as the failure\n",
    "        # Reason in the DescribeTrainingJob result.\n",
    "        trc = traceback.format_exc()\n",
    "        with open(os.path.join(output_path, 'failure'), 'w') as s:\n",
    "            s.write('Exception during training: ' + str(e) + '\\n' + trc)\n",
    "        # Printing this causes the exception to be in the training job logs\n",
    "        print(\n",
    "            'Exception during training: ' + str(e) + '\\n' + trc,\n",
    "            file=sys.stderr)\n",
    "        # A non-zero exit code causes the training job to be marked as Failed.\n",
    "        sys.exit(255)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()\n",
    "\n",
    "    # A zero exit code causes the job to be marked a Succeeded.\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本地端訓練（用 SageMaker 啟動的 Notebook Instance 上面的 docker 進行訓練工作）\n",
    "最後會輸出 .h5 檔就完成我們可以用來預測的 model file 了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Build Local ML Image\n",
    "!cp model/{model_name}.py model/train\n",
    "!chmod 777 model/train\n",
    "!docker build -t {model_name} .\n",
    "!docker run -v $(pwd)/local/$model_name:/opt/ml --rm {model_name} train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!ls -la local/{model_name}/model/model.h5\n",
    "!cp local/{model_name}/model/model.h5 ./model/{model_name}.h5\n",
    "!ls -la ./model/model_*.h5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
